# The default config file. You may override configs with `key=value` arguments on the CLI
# according to https://hydra.cc/docs/advanced/override_grammar/basic/.

# This defines the order in which configs are loaded. The latter configs
# override the earlier ones.
defaults:
  - _self_ # refers to this config file
  - model: llama-3-test # refers to model/llama-3-test.yaml
  - data: tokenized # refers to dataset/tokenized.yaml
  - trainer: llm # refers to trainer/llm.yaml

seed: 42
torch_dtype: float32

# logging and saving configuration
debug: false
project: prime-testing
name: test
notes: null


# set profile_start_step to a positive integer to enable profiling and start profiling 
# at that step. If profile_end_step is not set, profiling will continue until for 
# num_profile_steps (default 20) training steps or total step - 5 (to avoid issue #260)
# Also, try to make number of profile profile steps >= 10
profile_start_step: 3
profile_end_step: null

# The directory where profiling data will be stored. This might be overwritten 
# when using tp run to launch the run using XPK
profile_dir: ../profile


# The virtual device mesh shape to use within a TPU slice. This is also called
# the "ICI mesh", since devices within a slice enjoy a faster network called
# "Inter-Chip Interconnect".
ici_mesh:
  data: 1
  fsdp: 16
  tensor: 1
  expert: 1
  context: 1


# Shape of the logical mesh where each element is a TPU slice. This is called
# "Data Center Network (DCN) mesh" because TPU slices are usually connected
# together with slower data center networking, with the faster ICI network
# used within a slice.
#
# As an example, to enable 2-way data parallelism across 2 TPU slices, you may
# specify `dcn_mesh.data=2`.
dcn_mesh:
  data: 1
  fsdp: 1
  tensor: 1
  expert: 1
  context: 1


# These are default values for model activation rematerialization configuration.
# They can be overridden on the command line or by importing one of the presets
# in the `model/remat` directory.
model:
  # Name of classes in the module tree that are functionally pure.
  #
  # There are a few advantages of wrapping a module whose forward pass you know is
  # free of side-effects and whose behavior only depends on inputs in a `PureModule`:
  # - `PureModule`s will only be traced once.
  # - Framework profile scopes added via `xp.Trace` will show up in both the forward
  #   and the backward pass.
  pure_modules: []

  # Options for controlling tensor rematerialization.
  remat:
    # The class names of model layers whose intermediate activations should be
    # recomputed during the backward pass (i.e. activation checkpointing).
    activation_checkpoint_layers: []

    # If not null, compile a module of type `HomogeneousSequential` located at the
    # given path in the module tree using `torch_xla.experimental.scan_layers`.
    scan_layers: null

    # If specified, offload these tensors to host RAM during the forward pass and
    # move them back during the backward pass.
    #
    # The tensors to be offloaded should be given a name by wrapping them with the
    # `torchprime.torch_xla_models.offloading.offload_name` call. Then the same
    # name could be specified here to offload that tensor.
    #
    # Currently in order to offload tensors, `scan_layers` must also be enabled.
    offload_tensors: []
