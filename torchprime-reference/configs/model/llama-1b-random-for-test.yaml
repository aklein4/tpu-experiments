model_id: llama-1b-random-for-test
model_class: llama.LlamaForCausalLM  # Used to import the model from this class
pretrained_model: hf-internal-testing/tiny-random-LlamaForCausalLM
vocab_size: 32000
hidden_size: 16
intermediate_size: 64
num_hidden_layers: 2
num_attention_heads: 4
num_key_value_heads: 4
hidden_act: silu
max_position_embeddings: 2048
bos_token_id: 0
eos_token_id: 1
tokenizer_name: hf-internal-testing/tiny-random-llama
initializer_range: 0.02
rms_norm_eps: 1.0e-06
attention_dropout: false
attention_bias: false
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: null
rope_theta: 10000.0
