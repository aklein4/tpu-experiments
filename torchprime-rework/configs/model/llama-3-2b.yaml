defaults:
  - _self_  # refers to this config file
  - sharding: llama-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: llama  # refers to remat/llama.yaml

model_id: llama-3-2b
model_class: llama.LlamaForCausalLM  # Used to import the model from this class
pretrained_model: null
vocab_size: 128256
hidden_size: 3072
intermediate_size: 8192
num_hidden_layers: 22
num_attention_heads: 24
num_key_value_heads: 8
hidden_act: silu
max_position_embeddings: 2048
bos_token_id: 128000
eos_token_id: 128001
pad_token_id: 32000
tokenizer_name: meta-llama/Meta-Llama-3-8B
initializer_range: 0.02
rms_norm_eps: 1.0e-05
attention_dropout: false
attention_bias: false
# choose attention_kernel from: [flash_attention, splash_attention, null]
attention_kernel: flash_attention
rope_theta: 10000.0
